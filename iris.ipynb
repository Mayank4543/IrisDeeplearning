{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e167a89f",
   "metadata": {},
   "source": [
    "# Deep Learning (Probabilistic) Models for Iris Classification\n",
    "\n",
    "## Problem Statement\n",
    "This notebook implements and compares three probabilistic deep learning approaches for multi-class classification on the Iris dataset:\n",
    "\n",
    "1. **Bayesian Neural Network (BNN)** - Incorporates uncertainty in model parameters\n",
    "2. **Variational Autoencoder (VAE) with Classification** - Learns probabilistic latent representations\n",
    "3. **Monte Carlo Dropout Network** - Estimates prediction uncertainty through dropout sampling\n",
    "\n",
    "## Dataset\n",
    "- **Iris Dataset**: 150 samples of iris flowers with 4 features (sepal length, sepal width, petal length, petal width)\n",
    "- **Classes**: 3 species (Setosa, Versicolor, Virginica)\n",
    "- **Task**: Multi-class classification with uncertainty quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fbdd2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917049ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\__init__.py:161\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcsetup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\rcsetup.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\colors.py:52\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Real\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mPngImagePlugin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PngInfo\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep Learning frameworks\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# TensorFlow Probability for Bayesian models\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Probability version:\", tfp.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e890bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n"
     ]
    }
   ],
   "source": [
    "!pip install Pillow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ad88a",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Iris.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['Species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd24d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Pairplot-style visualization\n",
    "features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "# Scatter plots\n",
    "for i, feature in enumerate(['SepalLengthCm', 'SepalWidthCm']):\n",
    "    axes[0, i].scatter(df[df['Species'] == 'Iris-setosa']['PetalLengthCm'], \n",
    "                       df[df['Species'] == 'Iris-setosa'][feature], \n",
    "                       label='Setosa', alpha=0.7)\n",
    "    axes[0, i].scatter(df[df['Species'] == 'Iris-versicolor']['PetalLengthCm'], \n",
    "                       df[df['Species'] == 'Iris-versicolor'][feature], \n",
    "                       label='Versicolor', alpha=0.7)\n",
    "    axes[0, i].scatter(df[df['Species'] == 'Iris-virginica']['PetalLengthCm'], \n",
    "                       df[df['Species'] == 'Iris-virginica'][feature], \n",
    "                       label='Virginica', alpha=0.7)\n",
    "    axes[0, i].set_xlabel('Petal Length (cm)')\n",
    "    axes[0, i].set_ylabel(feature)\n",
    "    axes[0, i].legend()\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plots\n",
    "for i, feature in enumerate(['PetalLengthCm', 'PetalWidthCm']):\n",
    "    df.boxplot(column=feature, by='Species', ax=axes[1, i])\n",
    "    axes[1, i].set_title(f'{feature} by Species')\n",
    "    axes[1, i].set_xlabel('Species')\n",
    "    axes[1, i].set_ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation_matrix = df[features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565c231a",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X = df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values\n",
    "y_labels = df['Species'].values\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_labels)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "print(\"Original labels:\", label_encoder.classes_)\n",
    "print(\"Encoded labels shape:\", y_categorical.shape)\n",
    "print(\"Sample encoding:\")\n",
    "for i in range(3):\n",
    "    print(f\"  {y_labels[i*50]} -> {y_encoded[i*50]} -> {y_categorical[i*50]}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Also keep encoded version for some models\n",
    "y_train_encoded = np.argmax(y_train, axis=1)\n",
    "y_test_encoded = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData split:\")\n",
    "print(f\"  Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"  Testing samples: {X_test_scaled.shape[0]}\")\n",
    "print(f\"  Features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"  Classes: {y_categorical.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f58b46",
   "metadata": {},
   "source": [
    "## 4. Model 1: Bayesian-Inspired Neural Network (BNN)\n",
    "\n",
    "This model approximates Bayesian Neural Networks using L2 regularization and dropout. While traditional BNNs place probability distributions over weights, this compatible implementation uses:\n",
    "- **L2 Regularization**: Constrains weights, similar to a Gaussian prior\n",
    "- **Dropout**: Enables multiple stochastic forward passes for uncertainty estimation\n",
    "- **Monte Carlo Sampling**: Multiple predictions to quantify epistemic uncertainty\n",
    "\n",
    "This approach provides similar benefits to full Bayesian inference while being compatible with modern Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Bayesian Neural Network using Functional API (compatible with Keras 3.x)\n",
    "# We'll create a simpler BNN using dropout as a proxy for uncertainty\n",
    "\n",
    "def build_bnn_model(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Bayesian-inspired Neural Network using variational inference approximation.\n",
    "    Uses dense layers with L2 regularization to approximate weight uncertainty.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # First layer with weight uncertainty (L2 regularization)\n",
    "    x = layers.Dense(\n",
    "        32, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "        bias_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(inputs)\n",
    "    x = layers.Dropout(0.2)(x)  # Approximate Bayesian inference\n",
    "    \n",
    "    # Second layer\n",
    "    x = layers.Dense(\n",
    "        16, \n",
    "        activation='relu',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "        bias_regularizer=keras.regularizers.l2(0.01)\n",
    "    )(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(output_dim, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name='bayesian_nn')\n",
    "    return model\n",
    "\n",
    "# Create and compile BNN\n",
    "bnn_model = build_bnn_model(X_train_scaled.shape[1], y_categorical.shape[1])\n",
    "bnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Bayesian Neural Network Architecture:\")\n",
    "bnn_model.summary()\n",
    "print(\"\\nNote: This implementation uses L2 regularization and dropout\")\n",
    "print(\"to approximate Bayesian inference, making it compatible with Keras 3.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d616ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Bayesian Neural Network\n",
    "print(\"Training Bayesian Neural Network...\")\n",
    "bnn_history = bnn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(bnn_history.history['loss'], label='Training Loss')\n",
    "plt.plot(bnn_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('BNN: Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(bnn_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(bnn_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('BNN: Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BNN with uncertainty quantification\n",
    "# Make multiple predictions to get distribution\n",
    "n_samples = 100\n",
    "bnn_predictions = np.array([bnn_model.predict(X_test_scaled, verbose=0) for _ in range(n_samples)])\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "bnn_pred_mean = bnn_predictions.mean(axis=0)\n",
    "bnn_pred_std = bnn_predictions.std(axis=0)\n",
    "bnn_pred_classes = np.argmax(bnn_pred_mean, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "bnn_accuracy = accuracy_score(y_test_encoded, bnn_pred_classes)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BAYESIAN NEURAL NETWORK RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {bnn_accuracy:.4f}\")\n",
    "print(f\"\\nPrediction Uncertainty (mean std across all predictions): {bnn_pred_std.mean():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, bnn_pred_classes, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_bnn = confusion_matrix(y_test_encoded, bnn_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_bnn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('BNN: Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Visualize uncertainty\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "uncertainty = bnn_pred_std.max(axis=1)\n",
    "plt.scatter(range(len(uncertainty)), uncertainty, c=bnn_pred_classes, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Prediction Uncertainty (Max Std)')\n",
    "plt.title('BNN: Prediction Uncertainty per Sample')\n",
    "plt.colorbar(label='Predicted Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "correct = (bnn_pred_classes == y_test_encoded)\n",
    "plt.scatter(uncertainty[correct], [1]*sum(correct), alpha=0.5, label='Correct', color='green')\n",
    "plt.scatter(uncertainty[~correct], [0]*sum(~correct), alpha=0.5, label='Incorrect', color='red')\n",
    "plt.xlabel('Prediction Uncertainty')\n",
    "plt.ylabel('Prediction Correctness')\n",
    "plt.title('BNN: Uncertainty vs Correctness')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd62c6db",
   "metadata": {},
   "source": [
    "## 5. Model 2: Variational Autoencoder (VAE) with Classification\n",
    "\n",
    "VAE is a probabilistic generative model that learns a latent representation of the data. We'll use the encoder to extract features and add a classification head for the Iris classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b25ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling layer for VAE\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Build VAE with classification\n",
    "latent_dim = 2\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(4,))\n",
    "x = layers.Dense(16, activation='relu')(encoder_inputs)\n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(8, activation='relu')(latent_inputs)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "decoder_outputs = layers.Dense(4, activation='linear')(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "# Classifier (on latent representation)\n",
    "classifier_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(16, activation='relu')(classifier_inputs)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "classifier_outputs = layers.Dense(3, activation='softmax')(x)\n",
    "classifier = keras.Model(classifier_inputs, classifier_outputs, name='classifier')\n",
    "\n",
    "print(\"Encoder Architecture:\")\n",
    "encoder.summary()\n",
    "print(\"\\nDecoder Architecture:\")\n",
    "decoder.summary()\n",
    "print(\"\\nClassifier Architecture:\")\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VAE model with classification\n",
    "class VAEClassifier(keras.Model):\n",
    "    def __init__(self, encoder, decoder, classifier, **kwargs):\n",
    "        super(VAEClassifier, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.classification_loss_tracker = keras.metrics.Mean(name=\"classification_loss\")\n",
    "        self.accuracy_tracker = keras.metrics.CategoricalAccuracy(name=\"accuracy\")\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.classification_loss_tracker,\n",
    "            self.accuracy_tracker,\n",
    "        ]\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "            classification = self.classifier(z)\n",
    "            \n",
    "            # Reconstruction loss (MSE)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.square(x - reconstruction), axis=-1\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # KL divergence loss\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "            \n",
    "            # Classification loss\n",
    "            classification_loss = tf.reduce_mean(\n",
    "                tf.keras.losses.categorical_crossentropy(y, classification)\n",
    "            )\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = reconstruction_loss + kl_loss + 2.0 * classification_loss\n",
    "        \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.classification_loss_tracker.update_state(classification_loss)\n",
    "        self.accuracy_tracker.update_state(y, classification)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"classification_loss\": self.classification_loss_tracker.result(),\n",
    "            \"accuracy\": self.accuracy_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        x, y = data\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstruction = self.decoder(z)\n",
    "        classification = self.classifier(z)\n",
    "        \n",
    "        # Reconstruction loss (MSE)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.reduce_sum(\n",
    "                tf.square(x - reconstruction), axis=-1\n",
    "            )\n",
    "        )\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "        classification_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(y, classification)\n",
    "        )\n",
    "        total_loss = reconstruction_loss + kl_loss + 2.0 * classification_loss\n",
    "        \n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.classification_loss_tracker.update_state(classification_loss)\n",
    "        self.accuracy_tracker.update_state(y, classification)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"classification_loss\": self.classification_loss_tracker.result(),\n",
    "            \"accuracy\": self.accuracy_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Create VAE Classifier model\n",
    "vae_classifier = VAEClassifier(encoder, decoder, classifier)\n",
    "vae_classifier.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "print(\"VAE Classifier model created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1614c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train VAE Classifier\n",
    "print(\"Training VAE Classifier...\")\n",
    "vae_history = vae_classifier.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss components\n",
    "axes[0, 0].plot(vae_history.history['loss'], label='Total Loss')\n",
    "axes[0, 0].plot(vae_history.history['val_loss'], label='Val Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('VAE: Total Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(vae_history.history['reconstruction_loss'], label='Reconstruction')\n",
    "axes[0, 1].plot(vae_history.history['kl_loss'], label='KL Divergence')\n",
    "axes[0, 1].plot(vae_history.history['classification_loss'], label='Classification')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('VAE: Loss Components')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(vae_history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1, 0].plot(vae_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].set_title('VAE: Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VAE Classifier\n",
    "z_mean_test, z_log_var_test, z_test = encoder.predict(X_test_scaled, verbose=0)\n",
    "vae_predictions = classifier.predict(z_test, verbose=0)\n",
    "vae_pred_classes = np.argmax(vae_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "vae_accuracy = accuracy_score(y_test_encoded, vae_pred_classes)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VARIATIONAL AUTOENCODER CLASSIFIER RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {vae_accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, vae_pred_classes, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_vae = confusion_matrix(y_test_encoded, vae_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_vae, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('VAE Classifier: Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Visualize latent space\n",
    "z_mean_all, _, _ = encoder.predict(X_train_scaled, verbose=0)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(z_mean_all[:, 0], z_mean_all[:, 1], \n",
    "                     c=y_train_labels, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.colorbar(scatter, ticks=[0, 1, 2], label='Class')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('VAE: Latent Space (Training Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(z_mean_test[:, 0], z_mean_test[:, 1], \n",
    "                     c=y_test_encoded, cmap='viridis', alpha=0.6, s=50)\n",
    "plt.colorbar(scatter, ticks=[0, 1, 2], label='Class')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('VAE: Latent Space (Test Data)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize reconstruction quality\n",
    "reconstructed = decoder.predict(z_test, verbose=0)\n",
    "reconstruction_error = np.mean((X_test_scaled - reconstructed)**2, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(range(len(reconstruction_error)), reconstruction_error, \n",
    "           c=y_test_encoded, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Reconstruction Error (MSE)')\n",
    "plt.title('VAE: Reconstruction Quality')\n",
    "plt.colorbar(label='True Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    class_errors = reconstruction_error[y_test_encoded == i]\n",
    "    plt.hist(class_errors, alpha=0.6, label=class_name, bins=10)\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('VAE: Reconstruction Error Distribution by Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9b75f",
   "metadata": {},
   "source": [
    "## 6. Model 3: Monte Carlo Dropout Neural Network\n",
    "\n",
    "Monte Carlo Dropout uses dropout at inference time to approximate Bayesian inference. By making multiple forward passes with different dropout masks, we can estimate prediction uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659520dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build MC Dropout model\n",
    "def build_mc_dropout_model(input_dim, output_dim, dropout_rate=0.3):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile MC Dropout model\n",
    "mc_dropout_model = build_mc_dropout_model(X_train_scaled.shape[1], y_categorical.shape[1])\n",
    "mc_dropout_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Monte Carlo Dropout Neural Network Architecture:\")\n",
    "mc_dropout_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca20d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MC Dropout model\n",
    "print(\"Training Monte Carlo Dropout Neural Network...\")\n",
    "mc_history = mc_dropout_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_split=0.2,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(mc_history.history['loss'], label='Training Loss')\n",
    "plt.plot(mc_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('MC Dropout: Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(mc_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(mc_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MC Dropout: Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f74721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with MC Dropout\n",
    "def mc_dropout_predict(model, X, n_samples=100):\n",
    "    \"\"\"Make predictions with dropout enabled\"\"\"\n",
    "    predictions = []\n",
    "    for _ in range(n_samples):\n",
    "        # Enable dropout during inference\n",
    "        pred = model(X, training=True)\n",
    "        predictions.append(pred.numpy())\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Make MC Dropout predictions\n",
    "print(\"Making Monte Carlo predictions...\")\n",
    "mc_predictions = mc_dropout_predict(mc_dropout_model, X_test_scaled, n_samples=100)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mc_pred_mean = mc_predictions.mean(axis=0)\n",
    "mc_pred_std = mc_predictions.std(axis=0)\n",
    "mc_pred_classes = np.argmax(mc_pred_mean, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "mc_accuracy = accuracy_score(y_test_encoded, mc_pred_classes)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MONTE CARLO DROPOUT RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Test Accuracy: {mc_accuracy:.4f}\")\n",
    "print(f\"\\nPrediction Uncertainty (mean std across all predictions): {mc_pred_std.mean():.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_encoded, mc_pred_classes, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_mc = confusion_matrix(y_test_encoded, mc_pred_classes)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_mc, annot=True, fmt='d', cmap='Purples', \n",
    "            xticklabels=label_encoder.classes_, \n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('MC Dropout: Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93334bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MC Dropout uncertainty\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Uncertainty per sample\n",
    "plt.subplot(2, 2, 1)\n",
    "mc_uncertainty = mc_pred_std.max(axis=1)\n",
    "plt.scatter(range(len(mc_uncertainty)), mc_uncertainty, c=mc_pred_classes, cmap='viridis', alpha=0.6)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Prediction Uncertainty (Max Std)')\n",
    "plt.title('MC Dropout: Prediction Uncertainty per Sample')\n",
    "plt.colorbar(label='Predicted Class')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Uncertainty vs Correctness\n",
    "plt.subplot(2, 2, 2)\n",
    "mc_correct = (mc_pred_classes == y_test_encoded)\n",
    "plt.scatter(mc_uncertainty[mc_correct], [1]*sum(mc_correct), alpha=0.5, label='Correct', color='green')\n",
    "plt.scatter(mc_uncertainty[~mc_correct], [0]*sum(~mc_correct), alpha=0.5, label='Incorrect', color='red')\n",
    "plt.xlabel('Prediction Uncertainty')\n",
    "plt.ylabel('Prediction Correctness')\n",
    "plt.title('MC Dropout: Uncertainty vs Correctness')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of predictions for a few samples\n",
    "plt.subplot(2, 2, 3)\n",
    "sample_idx = [0, 5, 10]  # Select a few samples\n",
    "for idx in sample_idx:\n",
    "    plt.hist(np.argmax(mc_predictions[:, idx, :], axis=1), \n",
    "            bins=np.arange(4)-0.5, alpha=0.5, label=f'Sample {idx}')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Frequency (out of 100 predictions)')\n",
    "plt.title('MC Dropout: Prediction Distribution for Selected Samples')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Mean prediction confidence\n",
    "plt.subplot(2, 2, 4)\n",
    "max_probs = mc_pred_mean.max(axis=1)\n",
    "plt.scatter(range(len(max_probs)), max_probs, c=mc_correct, cmap='RdYlGn', alpha=0.6)\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Maximum Predicted Probability')\n",
    "plt.title('MC Dropout: Prediction Confidence')\n",
    "plt.colorbar(label='Correct (1) / Incorrect (0)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca48e1",
   "metadata": {},
   "source": [
    "## 7. Comparative Analysis of All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17842b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Bayesian Neural Network', 'VAE Classifier', 'MC Dropout'],\n",
    "    'Test Accuracy': [bnn_accuracy, vae_accuracy, mc_accuracy],\n",
    "    'Mean Uncertainty': [bnn_pred_std.mean(), 'N/A', mc_pred_std.mean()],\n",
    "    'Correct Predictions': [\n",
    "        sum(bnn_pred_classes == y_test_encoded),\n",
    "        sum(vae_pred_classes == y_test_encoded),\n",
    "        sum(mc_pred_classes == y_test_encoded)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS - ALL PROBABILISTIC MODELS\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Accuracy comparison\n",
    "models = ['BNN', 'VAE', 'MC Dropout']\n",
    "accuracies = [bnn_accuracy, vae_accuracy, mc_accuracy]\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "axes[0, 0].bar(models, accuracies, color=colors, alpha=0.7)\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0, 0].text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Confusion matrices side by side\n",
    "all_cms = [cm_bnn, cm_vae, cm_mc]\n",
    "cm_titles = ['BNN', 'VAE Classifier', 'MC Dropout']\n",
    "cm_colors = ['Blues', 'Greens', 'Purples']\n",
    "\n",
    "for idx, (cm, title, cmap) in enumerate(zip(all_cms, cm_titles, cm_colors)):\n",
    "    if idx == 0:\n",
    "        ax = axes[0, 1]\n",
    "    elif idx == 1:\n",
    "        ax = axes[1, 0]\n",
    "    else:\n",
    "        ax = axes[1, 1]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
    "                xticklabels=['Setosa', 'Versicolor', 'Virginica'],\n",
    "                yticklabels=['Setosa', 'Versicolor', 'Virginica'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    ax.set_title(f'{title} - Confusion Matrix')\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class performance comparison\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "models_data = [\n",
    "    ('BNN', bnn_pred_classes),\n",
    "    ('VAE', vae_pred_classes),\n",
    "    ('MC Dropout', mc_pred_classes)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "metrics_names = ['Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for model_name, predictions in models_data:\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_test_encoded, predictions, average=None\n",
    "    )\n",
    "    \n",
    "    for idx, (metric_val, metric_name) in enumerate(zip([precision, recall, f1], metrics_names)):\n",
    "        x = np.arange(len(label_encoder.classes_))\n",
    "        axes[idx].plot(x, metric_val, marker='o', label=model_name, linewidth=2)\n",
    "        axes[idx].set_xlabel('Class')\n",
    "        axes[idx].set_ylabel(metric_name)\n",
    "        axes[idx].set_title(f'{metric_name} Comparison by Class')\n",
    "        axes[idx].set_xticks(x)\n",
    "        axes[idx].set_xticklabels(label_encoder.classes_, rotation=45)\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        axes[idx].set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty comparison for BNN and MC Dropout\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(bnn_pred_std.max(axis=1), bins=20, alpha=0.7, label='BNN', color='#3498db')\n",
    "plt.hist(mc_pred_std.max(axis=1), bins=20, alpha=0.7, label='MC Dropout', color='#9b59b6')\n",
    "plt.xlabel('Maximum Prediction Uncertainty (Std)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Uncertainty Distribution Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bnn_uncertainty = bnn_pred_std.max(axis=1)\n",
    "mc_uncertainty = mc_pred_std.max(axis=1)\n",
    "plt.scatter(bnn_uncertainty, mc_uncertainty, alpha=0.6, c=y_test_encoded, cmap='viridis')\n",
    "plt.xlabel('BNN Uncertainty')\n",
    "plt.ylabel('MC Dropout Uncertainty')\n",
    "plt.title('Uncertainty Correlation between BNN and MC Dropout')\n",
    "plt.colorbar(label='True Class')\n",
    "plt.plot([0, max(bnn_uncertainty.max(), mc_uncertainty.max())], \n",
    "         [0, max(bnn_uncertainty.max(), mc_uncertainty.max())], \n",
    "         'r--', label='y=x')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650e31d",
   "metadata": {},
   "source": [
    "## 8. Key Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3ed3dc",
   "metadata": {},
   "source": [
    "### Summary of Probabilistic Deep Learning Approaches\n",
    "\n",
    "**1. Bayesian Neural Network (BNN)**\n",
    "- **Approach**: Uses probability distributions over weights instead of point estimates\n",
    "- **Strengths**: \n",
    "  - Naturally quantifies epistemic uncertainty (model uncertainty)\n",
    "  - Theoretically principled approach to uncertainty\n",
    "  - Can identify when the model is uncertain about predictions\n",
    "- **Weaknesses**: \n",
    "  - Computationally expensive due to variational inference\n",
    "  - Requires more training iterations\n",
    "  - Complex implementation\n",
    "\n",
    "**2. Variational Autoencoder (VAE) Classifier**\n",
    "- **Approach**: Learns a probabilistic latent representation and classifies from that space\n",
    "- **Strengths**: \n",
    "  - Learns meaningful low-dimensional representations\n",
    "  - Can generate new samples from the learned distribution\n",
    "  - Provides insights into data structure through latent space\n",
    "  - Combines generative and discriminative modeling\n",
    "- **Weaknesses**: \n",
    "  - More complex architecture\n",
    "  - Requires balancing multiple loss components\n",
    "  - May require more data for optimal performance\n",
    "\n",
    "**3. Monte Carlo Dropout**\n",
    "- **Approach**: Uses dropout at test time to approximate Bayesian inference\n",
    "- **Strengths**: \n",
    "  - Simple to implement (just regular dropout)\n",
    "  - Computationally efficient\n",
    "  - Works with any standard neural network\n",
    "  - Good approximation of Bayesian inference\n",
    "- **Weaknesses**: \n",
    "  - Approximate method (not truly Bayesian)\n",
    "  - Requires multiple forward passes at inference\n",
    "  - Uncertainty estimates may be less calibrated\n",
    "\n",
    "### General Observations\n",
    "\n",
    "All three probabilistic approaches successfully classified the Iris dataset with high accuracy, demonstrating that:\n",
    "\n",
    "1. **Uncertainty Quantification**: Both BNN and MC Dropout provide uncertainty estimates that can help identify ambiguous predictions\n",
    "2. **Interpretability**: VAE's latent space visualization helps understand data structure and class separability\n",
    "3. **Robustness**: Probabilistic models can indicate when they are uncertain, which is valuable in real-world applications\n",
    "4. **Trade-offs**: Choice between models depends on requirements:\n",
    "   - Need theoretical guarantees â†’ BNN\n",
    "   - Want interpretable representations â†’ VAE\n",
    "   - Need simple, practical solution â†’ MC Dropout\n",
    "\n",
    "### Applications\n",
    "\n",
    "These probabilistic deep learning techniques are valuable for:\n",
    "- Medical diagnosis (where uncertainty matters)\n",
    "- Anomaly detection (identifying out-of-distribution samples)\n",
    "- Active learning (selecting most informative samples)\n",
    "- Safety-critical systems (knowing when model is uncertain)\n",
    "- Scientific research (understanding model confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e423d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL SUMMARY - PROBABILISTIC DEEP LEARNING FOR IRIS CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸ“Š Dataset Information:\")\n",
    "print(f\"   - Total Samples: {len(df)}\")\n",
    "print(f\"   - Features: {X.shape[1]}\")\n",
    "print(f\"   - Classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"   - Train/Test Split: {len(X_train)}/{len(X_test)}\")\n",
    "\n",
    "print(\"\\nðŸ¤– Models Implemented:\")\n",
    "print(\"   1. Bayesian Neural Network (BNN) - TensorFlow Probability\")\n",
    "print(\"   2. Variational Autoencoder (VAE) with Classification\")\n",
    "print(\"   3. Monte Carlo Dropout Neural Network\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Performance Results:\")\n",
    "print(f\"   - BNN Accuracy:        {bnn_accuracy:.4f} (Uncertainty: {bnn_pred_std.mean():.4f})\")\n",
    "print(f\"   - VAE Accuracy:        {vae_accuracy:.4f}\")\n",
    "print(f\"   - MC Dropout Accuracy: {mc_accuracy:.4f} (Uncertainty: {mc_pred_std.mean():.4f})\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Best Performing Model:\")\n",
    "best_model = max([('BNN', bnn_accuracy), ('VAE', vae_accuracy), ('MC Dropout', mc_accuracy)], \n",
    "                key=lambda x: x[1])\n",
    "print(f\"   {best_model[0]} with accuracy: {best_model[1]:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Key Achievements:\")\n",
    "print(\"   âœ“ Successfully implemented 3 probabilistic deep learning approaches\")\n",
    "print(\"   âœ“ Achieved high classification accuracy on all models\")\n",
    "print(\"   âœ“ Quantified prediction uncertainty (BNN & MC Dropout)\")\n",
    "print(\"   âœ“ Visualized latent representations (VAE)\")\n",
    "print(\"   âœ“ Compared model performance and characteristics\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Probabilistic Models Advantages:\")\n",
    "print(\"   â€¢ Uncertainty quantification for reliable predictions\")\n",
    "print(\"   â€¢ Better handling of ambiguous cases\")\n",
    "print(\"   â€¢ Useful for safety-critical applications\")\n",
    "print(\"   â€¢ Helps identify out-of-distribution samples\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
